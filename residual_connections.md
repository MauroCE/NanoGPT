Skip connections come from [this](https://arxiv.org/abs/1512.03385) paper. The idea is that the sum operation distributes gradients equally. If we add these skip connecctions in our network, then these together create a "highway" for the gradient of the objective to get all the way to the input, undisturbed. At initialization then, objective gradients get all the way to the input and the transformer blocks are almost not there. But as time goes by, they become online and participate more in the learning process.