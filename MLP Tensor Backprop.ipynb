{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8547bdbd-88c1-4d31-9514-547cd4694521",
   "metadata": {},
   "source": [
    "The aim is to understand Backprop for tensors pretty well. Andrej argues that it's very important to avoiding shooting yourself in the foot. He wrote a [blogpost](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc4b277d-3d25-4530-83f9-4581475425f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182625, 3]) torch.Size([182625])\n",
      "torch.Size([22655, 3]) torch.Size([22655])\n",
      "torch.Size([22866, 3]) torch.Size([22866])\n"
     ]
    }
   ],
   "source": [
    "### content in this cell is the same as previous notebook Makemore MLP Better than ever\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Read data\n",
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "\n",
    "# Build vocabulary\n",
    "characters = sorted(list(set(''.join(names))))\n",
    "str_to_int = {s:i+1 for i, s in enumerate(characters)}\n",
    "str_to_int['.'] = 0.0\n",
    "int_to_str = {i:s for s, i in str_to_int.items()}\n",
    "vocab_size = len(int_to_str)\n",
    "\n",
    "block_size = 3  # context size\n",
    "\n",
    "def build_dataset(names):\n",
    "    X, Y = [], []\n",
    "    for name in names:\n",
    "        context = [0] * block_size\n",
    "        for character in name + \".\":\n",
    "            ix = str_to_int[character]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y, dtype=torch.long)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random \n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "n1 = int(0.8*len(names))\n",
    "n2 = int(0.9*len(names))\n",
    "\n",
    "Xtr, Ytr = build_dataset(names[:n1])\n",
    "Xdev, Ydev = build_dataset(names[n1:n2])\n",
    "Xte, Yte = build_dataset(names[n2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e90d365-bc3f-4eec-a68b-246945d362c9",
   "metadata": {},
   "source": [
    "# Function to Compare Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba4e5ca-b44c-44bf-a8d3-5a13e6fb4d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(s, dt, t):\n",
    "    \"\"\"Compares manual gradients to Pytorch gradients.\"\"\"\n",
    "    ex = torch.all(dt == t.grad).item()            # Exact gradient\n",
    "    app = torch.allclose(dt, t.grad)               # Approximate gradient\n",
    "    maxdiff = (dt - t.grad).abs().max().item()     # Maximum difference\n",
    "    print(f\"{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad552f1-7072-4dc5-bab6-9bc2ba350451",
   "metadata": {},
   "source": [
    "# Neural Network Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366d4edc-a797-4a5f-9faf-60e209d8343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_emb = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((vocab_size, n_emb), generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_emb*block_size, n_hidden), generator=g) * (5/3) / ((n_emb*block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g)  # Just for fun, it is useless due to BatchNorm\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "# Batch norm\n",
    "bngain = torch.randn((1, n_hidden), generator=g)*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden), generator=g)*0.1 \n",
    "\n",
    "# Some of these parameters are initialized in non-standard ways because sometimes initializing them \n",
    "# with all zeros can mask an incorrect implementation of the backward pass\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters))\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439a7d7-6131-4974-9894-44b866a9d975",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0723902c-7477-44e5-802d-31c244c8400e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size, ), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5bef68-8c13-4dfb-b541-1f55209f5023",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "728d0ed9-6d9b-49dd-8b24-ae9ac4de67ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da69fca4-20ba-4da6-9674-f796a330da77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
