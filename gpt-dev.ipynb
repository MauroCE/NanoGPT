{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd01aeb0-31cd-43e6-b25b-327a62842d70",
   "metadata": {},
   "source": [
    "# Read Tinyshakespeare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762bf8c1-c30d-4874-8b31-baa93557ddfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-22 18:13:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'input.txt'\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-05-22 18:13:32 (13.0 MB/s) - 'input.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you are using a Conda environment generated from scratch, you will need to run\n",
    "# `conda install jupyter` and `conda install wget`\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ba2e2b-e890-49e9-9dcb-3b83cd51e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715d269-da51-4d56-8647-ed2f120c6d55",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df48cd55-729b-49cf-944b-81f75b24159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size:  65\n"
     ]
    }
   ],
   "source": [
    "# Grab all unique characters in the text, sorted, and compute vocabulary size\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0852d-f5f7-4c3e-899e-79fd9f839298",
   "metadata": {},
   "source": [
    "# Encoder and Decoder for the Vocabulary\n",
    "Here we use the simplest possible schema for encoding/decoding: we simply map a string a list of the indices of its characters. There are much more complex schema. For instance:\n",
    "\n",
    "- Google uses [SentencePiece](https://github.com/google/sentencepiece), which is a **sub-word** tokenizer.\n",
    "- OpenAI uses [Tiktoken](https://github.com/openai/tiktoken) which is a fast Byte-Pair-Encoding tokenizer. This has `50257` tokens in the vocabulary. To use it, do `import tiktoken` and then `enc = tiktoken.get_encoding('gpt2')` and then `enc.n_vocab`.\n",
    "\n",
    "Here we use a simple character-level tokenizer, which means that our **codebook** (here called `char`) has a very small size, only `65` possible characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5af22e0-a481-4be6-aab5-0d517b8ca533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries mapping characters to their index and vice versa\n",
    "str_to_int = {character: integer for integer, character in enumerate(chars)}\n",
    "int_to_str = {integer: character for integer, character in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622016bb-0387-470c-a04a-0e3f9aec51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder functions\n",
    "encode = lambda string: [str_to_int[character] for character in string]  # string --> list(int)\n",
    "decode = lambda intlist: ''.join([int_to_str[integer] for integer in intlist])  # list(int) --> string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a8c281-dff1-4182-875d-eba5363b5d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder and decoder functions\n",
    "test_text = \"Hello, World!\"\n",
    "print(encode(test_text))\n",
    "print(decode(encode(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa700ab-9119-408d-aafd-4be8cbc47b14",
   "metadata": {},
   "source": [
    "# Tokenize TinyShakespeare\n",
    "This requires not only pytorch but also numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29b71aa-c809-48c9-a659-e78ffa2a58eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  torch.Size([1115394])  Data dtype:  torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Encode TinyShakespeare and store in a PyTorch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"Data shape: \", data.shape, \" Data dtype: \", data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d92926-5bb4-4e1a-ac4c-62bbb4f7dd4a",
   "metadata": {},
   "source": [
    "# Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d8fa68-ea7c-4eda-8e9e-7ff5938e8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]  # 90% training\n",
    "val_data = data[n:]    # 10% validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d955cd2-d3ca-4b96-98d4-619c7b33cb51",
   "metadata": {},
   "source": [
    "# Context size and traning examples\n",
    "Andrey Karpathy calls the `context size` with a different name: the `block_size`. Notice that, being a sequence model, inside a sequence of length `m` there are `m-1` examples. We take full advantage of this when training transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbd1e7d-ce11-430d-bb12-351c85459a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8  # context size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf631ae8-51d2-4088-b5c5-35a75fe09b95",
   "metadata": {},
   "source": [
    "This can be easily spelled out by printing the training examples contained in a small training block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d59c1c-d5e1-494d-b4b7-92d5743a91db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18] Target: 47\n",
      "Context: [18, 47] Target: 56\n",
      "Context: [18, 47, 56] Target: 57\n",
      "Context: [18, 47, 56, 57] Target: 58\n",
      "Context: [18, 47, 56, 57, 58] Target: 1\n",
      "Context: [18, 47, 56, 57, 58, 1] Target: 15\n",
      "Context: [18, 47, 56, 57, 58, 1, 15] Target: 47\n",
      "Context: [18, 47, 56, 57, 58, 1, 15, 47] Target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]     # 0,...,8 (context)\n",
    "y = train_data[1:block_size+1]  # 1,...,9 (targets) off-set by one\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context.tolist()} Target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059be60-c380-4626-a44b-142a8666fc55",
   "metadata": {},
   "source": [
    "# Batching\n",
    "In practice we want to perform operations in parallel, and this requires the notion of a batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42168234-c321-4369-b563-44a51fe766e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4  # Number of independent sequences processed in parallel\n",
    "block_size = 8  # Maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Generates batch of data of inputs `x` and targets `y`.\"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # Sample 4 integers from [0, n-block_size], representing off-sets, one for each batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    # Grab context and target\n",
    "    context = torch.stack([data[i:i+block_size] for i in ix])  # (batch_size, block_size) = (4, 8)\n",
    "    targets = torch.stack([data[i+1:i+block_size+1] for i in ix])  # (batch_size, block_size) = (4, 8)\n",
    "    return context, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d7dfbc9-06c4-4f33-8149-c9fccffa9e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "Context: [24] Target: 43\n",
      "Context: [24, 43] Target: 58\n",
      "Context: [24, 43, 58] Target: 5\n",
      "Context: [24, 43, 58, 5] Target: 57\n",
      "Context: [24, 43, 58, 5, 57] Target: 1\n",
      "Context: [24, 43, 58, 5, 57, 1] Target: 46\n",
      "Context: [24, 43, 58, 5, 57, 1, 46] Target: 43\n",
      "Context: [24, 43, 58, 5, 57, 1, 46, 43] Target: 39\n",
      "Context: [44] Target: 53\n",
      "Context: [44, 53] Target: 56\n",
      "Context: [44, 53, 56] Target: 1\n",
      "Context: [44, 53, 56, 1] Target: 58\n",
      "Context: [44, 53, 56, 1, 58] Target: 46\n",
      "Context: [44, 53, 56, 1, 58, 46] Target: 39\n",
      "Context: [44, 53, 56, 1, 58, 46, 39] Target: 58\n",
      "Context: [44, 53, 56, 1, 58, 46, 39, 58] Target: 1\n",
      "Context: [52] Target: 58\n",
      "Context: [52, 58] Target: 1\n",
      "Context: [52, 58, 1] Target: 58\n",
      "Context: [52, 58, 1, 58] Target: 46\n",
      "Context: [52, 58, 1, 58, 46] Target: 39\n",
      "Context: [52, 58, 1, 58, 46, 39] Target: 58\n",
      "Context: [52, 58, 1, 58, 46, 39, 58] Target: 1\n",
      "Context: [52, 58, 1, 58, 46, 39, 58, 1] Target: 46\n",
      "Context: [25] Target: 17\n",
      "Context: [25, 17] Target: 27\n",
      "Context: [25, 17, 27] Target: 10\n",
      "Context: [25, 17, 27, 10] Target: 0\n",
      "Context: [25, 17, 27, 10, 0] Target: 21\n",
      "Context: [25, 17, 27, 10, 0, 21] Target: 1\n",
      "Context: [25, 17, 27, 10, 0, 21, 1] Target: 54\n",
      "Context: [25, 17, 27, 10, 0, 21, 1, 54] Target: 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(\"Context: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets: \")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"Context: {context.tolist()} Target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54664298-1378-4404-8fef-4e472e703386",
   "metadata": {},
   "source": [
    "Therefore, our input to the transformer would be the `(batch_size, block_size)` array `xb` shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46792be4-3d43-4db5-86c7-10c8c1d02f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386603f-62ff-4ff0-a2c7-d0445f6ca522",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68086f6d-9c51-4b37-afb2-d033a8c4971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"Bigram model, see Karpathy's previous series of videos.\"\"\"\n",
    "        super().__init__()\n",
    "        # Tokens read off the logits for the next token from a lookup table\n",
    "        # Token embedding table has size (vocab_size, vocab_size)\n",
    "        # The way it works is that the input, say 24 (the first one in xb above) will take the 24th row of this\n",
    "        # embedding table. \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass. Takes `idx` and `targets` which are both `(B, T)` tensors of integers.\n",
    "        Here `B` is the batch_size and `T` should be the block/context length.\"\"\"\n",
    "        # PyTorch will grab the row corresponding to the indices provided and return logits in\n",
    "        # the shape (batch, time, channel). Here batch=4, time=8, channel=65 (vocab size)\n",
    "        # The logits here are like the scores for the next token in the sequence\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C)\n",
    "\n",
    "        # Negative log-likelihood loss (cross-entropy). Importantly, when working with multi-dimensional inputs, \n",
    "        # PyTorch's documentation https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        # mentions that it requires dimensions (B, C, T) using our notation. A simpler alternative is to simply shape it to (B*T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Here `idx` is the current context of tokens in some batch, so it is `(B, T)`. This function will continue the generation\n",
    "        one by one, for both the B and T dimensions. It keeps doing this until max_new_tokens.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)   # Get the predictions (calls forward(idx, targets=None))\n",
    "            logits = logits[:, -1, :]  # (B, T, C) --> (B, C) we focus only on the last \"time step\" (last token in the context)\n",
    "            probs = F.softmax(logits, dim=-1)  # Use Softmax to get probabilities. (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample using the probabilities (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # append the sampled index to the running sequence (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "bigram = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigram(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c959b3-8ea0-4f68-80a3-d0f51d92931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# Sample from the model\n",
    "print(\n",
    "    decode(\n",
    "        bigram.generate(\n",
    "            idx=torch.zeros((1, 1), dtype=torch.long),\n",
    "            max_new_tokens=100\n",
    "        )[0].tolist()   # use [0] to pluck out the single batch dimension (we are sending in [[0]] with B=T=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cf96c-48ea-41a7-be26-61d3d162b644",
   "metadata": {},
   "source": [
    "Of course, this is silly. We are feeding in an entire context but really just using the last token to predict the next (see `logits[:, -1, :]`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26fd750-c613-442d-9093-a72802961ab6",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3504eda1-e67e-46a4-bb76-514c35b723f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bigram.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8f60afc-035e-45ca-bd5e-45f024f3dff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4955925941467285\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = bigram(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1116e-7ba5-4375-9582-948a8cca0c79",
   "metadata": {},
   "source": [
    "# Generate After Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa886f95-0892-4738-8d45-99f05c7e2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BOMy is, mumot me bthenindsoferlle cardethe le h. sps t theleriny hacl fougarke,\n",
      "\n",
      "Angllll a hieald lo d,\n",
      "Thade bof jak rend\n",
      "Thid weme w, aithefithe h tes s tomeeseanmpotl'de y l, t,\n",
      "Goutandos t tof al o ad, prsthoneirermenicisull gsewd o re, myofary pef s'HNORicepim th, thit y h,\n",
      "Cothor nsethat bre, lly, farotodis f vel cld, minounged tithit:\n",
      "O:\n",
      "\n",
      "And;\n",
      "DAppr.\n",
      "\n",
      "VI cerer e: t\n",
      "Wipe ght cou FLOLOF ve\n"
     ]
    }
   ],
   "source": [
    "# Sample from the model\n",
    "print(\n",
    "    decode(\n",
    "        bigram.generate(\n",
    "            idx=torch.zeros((1, 1), dtype=torch.long),\n",
    "            max_new_tokens=400\n",
    "        )[0].tolist()   # use [0] to pluck out the single batch dimension (we are sending in [[0]] with B=T=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7e3067-af2f-4937-9228-558a48cc15d7",
   "metadata": {},
   "source": [
    "# Mathematical Trick of Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "601744c6-528f-4b30-8d4a-d6a13f0f82e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "B, T, C = 4, 8, 2   # batch, time, channels\n",
    "\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a79db4d-2c4b-4e8a-acf6-cd22c8939595",
   "metadata": {},
   "source": [
    "We only want information to flow forward: we don't want a token at position 5 to get information from token at position 6, since we are trying to predict that. Instead we want it to receive information from tokens 4, 3, 2, 1. A simple way to transfer information is just the mean of the previous tokens. This is not self-attention, but at least we will get used to not having information flowing backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0eb80b0-f4cc-4648-be62-a779d1dc735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b, t] = mean_{i <= t} x[b, i]\n",
    "# bow = \"bag of words\" it's the term used when people are just averaging out things\n",
    "xbow = torch.zeros((B, T, C))\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1]  # (t, C), grab current batch, up to and including location t\n",
    "        xbow[b, t] = torch.mean(xprev, 0) # compute the mean over the time i.e. context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6a52f-e70c-470c-9c54-237649fc86cf",
   "metadata": {},
   "source": [
    "This is very inefficient, there is a better way of doing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c8407c6-2d45-4522-bbd0-b347798bbd51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b = \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c = \n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "a = torch.ones(3, 3)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c = \")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b3a23f7-251b-4742-a064-57b22c54c139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "--\n",
      "b = \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c = \n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# If we use the lower-triangular part, we only get the running sums\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c = \")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35ec3f-7345-42bb-9c4b-173b7aa93360",
   "metadata": {},
   "source": [
    "To get the average, we simply normalise the rows beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6171132b-67c2-45c4-a000-53c64ed7700b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "--\n",
      "b = \n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "--\n",
      "c = \n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# If we use the lower-triangular part, we only get the running sums\n",
    "torch.manual_seed(42)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "a /= a.sum(1, keepdim=True)\n",
    "b = torch.randint(0, 10, (3, 2)).float()\n",
    "c = a @ b\n",
    "\n",
    "print(\"a = \")\n",
    "print(a)\n",
    "print(\"--\")\n",
    "print(\"b = \")\n",
    "print(b)\n",
    "print(\"--\")\n",
    "print(\"c = \")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2847d4-f475-4409-a083-5d7472bec63c",
   "metadata": {},
   "source": [
    "Now we can do the initial calculation much more quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "48114222-f1e8-4add-8fb5-150ccd89afd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### THIS \n",
    "# xbow = torch.zeros((B, T, C))\n",
    "# for b in range(B):\n",
    "#     for t in range(T):\n",
    "#         xprev = x[b, :t+1] \n",
    "#         xbow[b, t] = torch.mean(xprev, 0)\n",
    "\n",
    "#### BECOMES THIS\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "# Here pytorch will do batch matrix multiplication meaning that the (T, T) matrix will\n",
    "# multiply each of the (T, C) matrices (there are B of them) in `weights`\n",
    "xbow2 = weights @ x  # (T, T) @ (B, T, C) --> (B, T, C)\n",
    "torch.allclose(xbow, xbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41710da6-0a81-4807-9514-7b0a03d65734",
   "metadata": {},
   "source": [
    "There is a different version of doing this: with softmax. Softmax does the normalization, but the advantage of this point of view is that while here we are choosing a zero `wei` matrix ourselves, in practice this would be a matrix of interactions that would be learned from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0ca010c-8166-488e-916c-6d4d8ab680bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))  # for all elements where tril==0, make them -inf\n",
    "wei = F.softmax(wei, dim=1)  # normalization.\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdaf3c3-421b-42b5-b4f6-4f980265db28",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347c6124-8a74-45fb-97eb-7273870418ec",
   "metadata": {},
   "source": [
    "Each token will emit two vectors: a **query** and a **key**.\n",
    "\n",
    "- Query: \"What am I looking for?\"\n",
    "- Key: \"What do I contain?\"\n",
    "\n",
    "To obtain affinities, we do dot products between keys and queries. Basically, my query times all the keys, will become our `wei`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "97f08bd5-252a-40c6-b1d6-45b5820891d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Let's see a single head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)   # These are just matrix multiplies with matrix of size (C, head_size)\n",
    "query = nn.Linear(C, head_size, bias=False)  # (C, head_size)\n",
    "value = nn.Linear(C, head_size, bias=False)  # (C, head_size)\n",
    "\n",
    "# Apply key and query forward to x, to obtain `k` and `q`\n",
    "# Notice this happens fully independently, no communication has happened. Each token\n",
    "# has produced keys and queries independently\n",
    "k = key(x)    # (B, T, head_size) since (B, T, C) @ (C, head_size) = (B, T, head_size)\n",
    "q = query(x)  # (B, T, head_size)\n",
    "\n",
    "# Compute the affinities \n",
    "# Basically, each key/query vector has size `head_size`, which in this case is `16`. We want to multiply\n",
    "# each query with each key, but we need to use transpose carefully because we have batch dimension. \n",
    "# (B, T, hs) @ (B, hs, T) performs batched matrix multiplication, meaning that for each b=0, ..., B-1\n",
    "# we compute the matrix multiplication q[b] @ k[b].T. This means\n",
    "# (B, T, hs) @ (B, hs, T) = (B, T, T)\n",
    "wei = q @ k.transpose(-2, -1)  # (B, T, T)\n",
    "# Basically, each (T, T) matrix contains affinities for that batch\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "# Importantly, we don't just compute `wei @ x` but we do it with the value of `x`\n",
    "# Remember `x` has shape `(B, T, C)` and value matrix has shape `(C, head_size)` so `value(x)` has shape `(B, T, head_size)`\n",
    "v = value(x)\n",
    "out = wei @ v  # basically we aggregate v, not the raw x, meaning the output will have size `(B, T, head_size)` !!!!!!!\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dac282-2c07-4b6d-9de9-c9f3b7b22f18",
   "metadata": {},
   "source": [
    "Intuition: `x` contains the information and position about tokens, this is private information. \n",
    "\n",
    "- Query: what I am interested in\n",
    "- Key: what I have\n",
    "- Value: This is what I will communicate to you, if you find it interesting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fe94e3-722c-4842-98bc-1b98483c9c0b",
   "metadata": {},
   "source": [
    "Notice that there is no concept of space. Self-attention behaves on a set of vectors, but contains no positional information. That is why we do the positional encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5152a87c-17c0-4ad2-a7e7-c713fa130bae",
   "metadata": {},
   "source": [
    "Attention is just a **communication mechanism**. \n",
    "\n",
    "> The elements across the batch dimension (which are independent examples) never talk to each other. We process them all independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038180fb-0f48-401d-a3f8-2faf2060dcf7",
   "metadata": {},
   "source": [
    "**Encoder block**:\n",
    "\n",
    "- Encoder block simply means we remove the line `wei = wei.masked_fill(tril == 0, float('-inf'))` and we allow all the nodes to completely talk to each other. \n",
    "\n",
    "**Decoder block**:\n",
    "\n",
    "- It's the one we have implemented here, with the masking. It is called decoder because it is \"decoding\" language and has this autoregressive structure.\n",
    "\n",
    "**Cross attention**:\n",
    "\n",
    "- The reason why what we have implemented earlier is called **self-attention** is that we compute key query and values from the vector itself, i.e. `key(x)`, `query(x)` and `value(x)`. For instance, in a **encoder-decoder** transformer, you have that the `queries` are computed from `x` but `keys` and `values` come from a separate source. Sometimes from an encoder block which encodes some context that we would like to condition on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4de456-5a5a-4b21-8ec9-341a1de157df",
   "metadata": {},
   "source": [
    "SCALED ATTENTION\n",
    "\n",
    "> In the \"Attention is all you need paper\" they use\n",
    "$$\n",
    "\\frac{QK^\\top}{\\sqrt{d_k}}\n",
    "$$\n",
    "> where $d_k$ is the `head_size`. This is called `scaled_attention`. We'd implement it as `wei = q @ k.transpose(-2, -1) * head_size**-0.5`\n",
    "\n",
    "\n",
    "The reason why this is done is that if you initialize `q` and `k` to be Gaussians, then the variance of the weights will be `head_size`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7e62f84-55a4-4c58-b1e2-db19e332e943",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39d7ce69-7119-4013-bef7-c48cd2a081de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0966), tensor(0.9416), tensor(16.1036))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a93a6-23fe-41ec-9234-94dd56ad6cb9",
   "metadata": {},
   "source": [
    "Whereas if we normalize, this is no longer true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1674ef8d-ab8e-4b11-93e2-8d09e8054ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac27618c-5ba4-4bb4-84eb-0fc625f7e790",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0104), tensor(1.0204), tensor(1.1053))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var(), q.var(), wei.var()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3bfd9e-df5d-492a-8c65-66b7ca49fbc4",
   "metadata": {},
   "source": [
    "Since `wei` feeds into a softmax, it is important that at initialization it is fairly diffuse, otherwise softmax will converge to one-hot vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d1192-dade-42e8-af72-e5d4e5fe3d4f",
   "metadata": {},
   "source": [
    "# Layer Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86017d01-503a-4653-a295-6846210526cd",
   "metadata": {},
   "source": [
    "### Recall Batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9b6c7483-624c-4fba-8222-5493d2d74ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        # the self.training attribute is actually present in Pytorch too because many layers have a different \n",
    "        # behavior based on whether you are during training or inference\n",
    "        self.training = True\n",
    "        # Parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "        # Buffers (trained with a running momentum update)\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_var = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Calculate forward pass differently if we are in training or inference mode\n",
    "        if self.training:\n",
    "            # During training we estimate the from the batch\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            # During inference we use the running ones\n",
    "            xmean = self.running_mean\n",
    "            xvar = self.running_var\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma*xhat + self.beta\n",
    "        # Update buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (1 - self.momentum)*self.running_mean + self.momentum*xmean\n",
    "                self.running_var = (1 - self.momentum)*self.running_var + self.momentum*xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "303b2c3a-1ebf-47a4-84ac-4e5a143c0455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "module = BatchNorm1d(100)\n",
    "x = torch.randn(32, 100)   # batch=32, dim=100\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b6904a05-536b-4dc4-a69c-6bf0ec3f8ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.4901e-08), tensor(1.0000))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this makes sure that each column has mean 0 and std 1\n",
    "x[:, 0].mean(), x[:, 0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "15f88b67-407b-4f81-9bd6-4ef13ac8e9ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0411), tensor(1.0431))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rows are not normalized!\n",
    "x[0, :].mean(), x[0, :].std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd0f778-4c7d-428b-9355-5a0eeb99b7a0",
   "metadata": {},
   "source": [
    "### Layer Norm\n",
    "We literally just change \n",
    "```\n",
    "xmean = x.mean(0, keepdim=True)\n",
    "xvar = x.var(0, keepdim=True)\n",
    "```\n",
    "to \n",
    "```\n",
    "xmean = x.mean(1, keepdim=True)\n",
    "xvar = x.var(1, keepdim=True)\n",
    "```\n",
    "\n",
    "IMPORTANTLY, since this is done independently for each example and does not comput it mean and std across batches, we can remove the `torch.no_grad()` stuff. There is no distinction between training and test time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0aae3b6c-c34b-47b3-bb26-bc1c8155cacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm1d:\n",
    "\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        self.eps = eps\n",
    "        # Parameters (trained with backprop)\n",
    "        self.gamma = torch.ones(dim)\n",
    "        self.beta = torch.zeros(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Calculate forward pass differently if we are in training or inference mode\n",
    "        xmean = x.mean(1, keepdim=True)\n",
    "        xvar = x.var(1, keepdim=True)\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        self.out = self.gamma*xhat + self.beta\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "557b6f5e-98a5-4870-b521-1bb1d2b46e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100)   # batch=32, dim=100\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8a9d72ca-cb96-4a16-9674-41597368051c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.5763e-09), tensor(1.0000))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOW ROWS ARE NORMALIZED!\n",
    "x[0, :].mean(), x[0, :].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8761338e-5136-4253-b32a-5c480e10ed65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
