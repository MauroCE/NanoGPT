{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd01aeb0-31cd-43e6-b25b-327a62842d70",
   "metadata": {},
   "source": [
    "# Read Tinyshakespeare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "762bf8c1-c30d-4874-8b31-baa93557ddfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-05-22 18:13:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'input.txt'\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
      "\n",
      "2024-05-22 18:13:32 (13.0 MB/s) - 'input.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# If you are using a Conda environment generated from scratch, you will need to run\n",
    "# `conda install jupyter` and `conda install wget`\n",
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70ba2e2b-e890-49e9-9dcb-3b83cd51e092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9715d269-da51-4d56-8647-ed2f120c6d55",
   "metadata": {},
   "source": [
    "# Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df48cd55-729b-49cf-944b-81f75b24159e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "Vocabulary size:  65\n"
     ]
    }
   ],
   "source": [
    "# Grab all unique characters in the text, sorted, and compute vocabulary size\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(\"Vocabulary size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a0852d-f5f7-4c3e-899e-79fd9f839298",
   "metadata": {},
   "source": [
    "# Encoder and Decoder for the Vocabulary\n",
    "Here we use the simplest possible schema for encoding/decoding: we simply map a string a list of the indices of its characters. There are much more complex schema. For instance:\n",
    "\n",
    "- Google uses [SentencePiece](https://github.com/google/sentencepiece), which is a **sub-word** tokenizer.\n",
    "- OpenAI uses [Tiktoken](https://github.com/openai/tiktoken) which is a fast Byte-Pair-Encoding tokenizer. This has `50257` tokens in the vocabulary. To use it, do `import tiktoken` and then `enc = tiktoken.get_encoding('gpt2')` and then `enc.n_vocab`.\n",
    "\n",
    "Here we use a simple character-level tokenizer, which means that our **codebook** (here called `char`) has a very small size, only `65` possible characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5af22e0-a481-4be6-aab5-0d517b8ca533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionaries mapping characters to their index and vice versa\n",
    "str_to_int = {character: integer for integer, character in enumerate(chars)}\n",
    "int_to_str = {integer: character for integer, character in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622016bb-0387-470c-a04a-0e3f9aec51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder and Decoder functions\n",
    "encode = lambda string: [str_to_int[character] for character in string]  # string --> list(int)\n",
    "decode = lambda intlist: ''.join([int_to_str[integer] for integer in intlist])  # list(int) --> string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15a8c281-dff1-4182-875d-eba5363b5d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20, 43, 50, 50, 53, 6, 1, 35, 53, 56, 50, 42, 2]\n",
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "# Test the encoder and decoder functions\n",
    "test_text = \"Hello, World!\"\n",
    "print(encode(test_text))\n",
    "print(decode(encode(test_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa700ab-9119-408d-aafd-4be8cbc47b14",
   "metadata": {},
   "source": [
    "# Tokenize TinyShakespeare\n",
    "This requires not only pytorch but also numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29b71aa-c809-48c9-a659-e78ffa2a58eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape:  torch.Size([1115394])  Data dtype:  torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Encode TinyShakespeare and store in a PyTorch tensor\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"Data shape: \", data.shape, \" Data dtype: \", data.dtype)\n",
    "print(data[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d92926-5bb4-4e1a-ac4c-62bbb4f7dd4a",
   "metadata": {},
   "source": [
    "# Train and Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80d8fa68-ea7c-4eda-8e9e-7ff5938e8867",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(0.9*len(data))\n",
    "\n",
    "train_data = data[:n]  # 90% training\n",
    "val_data = data[n:]    # 10% validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d955cd2-d3ca-4b96-98d4-619c7b33cb51",
   "metadata": {},
   "source": [
    "# Context size and traning examples\n",
    "Andrey Karpathy calls the `context size` with a different name: the `block_size`. Notice that, being a sequence model, inside a sequence of length `m` there are `m-1` examples. We take full advantage of this when training transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afbd1e7d-ce11-430d-bb12-351c85459a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 8  # context size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf631ae8-51d2-4088-b5c5-35a75fe09b95",
   "metadata": {},
   "source": [
    "This can be easily spelled out by printing the training examples contained in a small training block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d59c1c-d5e1-494d-b4b7-92d5743a91db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [18] Target: 47\n",
      "Context: [18, 47] Target: 56\n",
      "Context: [18, 47, 56] Target: 57\n",
      "Context: [18, 47, 56, 57] Target: 58\n",
      "Context: [18, 47, 56, 57, 58] Target: 1\n",
      "Context: [18, 47, 56, 57, 58, 1] Target: 15\n",
      "Context: [18, 47, 56, 57, 58, 1, 15] Target: 47\n",
      "Context: [18, 47, 56, 57, 58, 1, 15, 47] Target: 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]     # 0,...,8 (context)\n",
    "y = train_data[1:block_size+1]  # 1,...,9 (targets) off-set by one\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"Context: {context.tolist()} Target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5059be60-c380-4626-a44b-142a8666fc55",
   "metadata": {},
   "source": [
    "# Batching\n",
    "In practice we want to perform operations in parallel, and this requires the notion of a batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42168234-c321-4369-b563-44a51fe766e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "batch_size = 4  # Number of independent sequences processed in parallel\n",
    "block_size = 8  # Maximum context length for predictions\n",
    "\n",
    "def get_batch(split):\n",
    "    \"\"\"Generates batch of data of inputs `x` and targets `y`.\"\"\"\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    # Sample 4 integers from [0, n-block_size], representing off-sets, one for each batch\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
    "    # Grab context and target\n",
    "    context = torch.stack([data[i:i+block_size] for i in ix])  # (batch_size, block_size) = (4, 8)\n",
    "    targets = torch.stack([data[i+1:i+block_size+1] for i in ix])  # (batch_size, block_size) = (4, 8)\n",
    "    return context, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d7dfbc9-06c4-4f33-8149-c9fccffa9e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: \n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "Targets: \n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "Context: [24] Target: 43\n",
      "Context: [24, 43] Target: 58\n",
      "Context: [24, 43, 58] Target: 5\n",
      "Context: [24, 43, 58, 5] Target: 57\n",
      "Context: [24, 43, 58, 5, 57] Target: 1\n",
      "Context: [24, 43, 58, 5, 57, 1] Target: 46\n",
      "Context: [24, 43, 58, 5, 57, 1, 46] Target: 43\n",
      "Context: [24, 43, 58, 5, 57, 1, 46, 43] Target: 39\n",
      "Context: [44] Target: 53\n",
      "Context: [44, 53] Target: 56\n",
      "Context: [44, 53, 56] Target: 1\n",
      "Context: [44, 53, 56, 1] Target: 58\n",
      "Context: [44, 53, 56, 1, 58] Target: 46\n",
      "Context: [44, 53, 56, 1, 58, 46] Target: 39\n",
      "Context: [44, 53, 56, 1, 58, 46, 39] Target: 58\n",
      "Context: [44, 53, 56, 1, 58, 46, 39, 58] Target: 1\n",
      "Context: [52] Target: 58\n",
      "Context: [52, 58] Target: 1\n",
      "Context: [52, 58, 1] Target: 58\n",
      "Context: [52, 58, 1, 58] Target: 46\n",
      "Context: [52, 58, 1, 58, 46] Target: 39\n",
      "Context: [52, 58, 1, 58, 46, 39] Target: 58\n",
      "Context: [52, 58, 1, 58, 46, 39, 58] Target: 1\n",
      "Context: [52, 58, 1, 58, 46, 39, 58, 1] Target: 46\n",
      "Context: [25] Target: 17\n",
      "Context: [25, 17] Target: 27\n",
      "Context: [25, 17, 27] Target: 10\n",
      "Context: [25, 17, 27, 10] Target: 0\n",
      "Context: [25, 17, 27, 10, 0] Target: 21\n",
      "Context: [25, 17, 27, 10, 0, 21] Target: 1\n",
      "Context: [25, 17, 27, 10, 0, 21, 1] Target: 54\n",
      "Context: [25, 17, 27, 10, 0, 21, 1, 54] Target: 39\n"
     ]
    }
   ],
   "source": [
    "xb, yb = get_batch('train')\n",
    "print(\"Context: \")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print(\"Targets: \")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"Context: {context.tolist()} Target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54664298-1378-4404-8fef-4e472e703386",
   "metadata": {},
   "source": [
    "Therefore, our input to the transformer would be the `(batch_size, block_size)` array `xb` shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46792be4-3d43-4db5-86c7-10c8c1d02f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8386603f-62ff-4ff0-a2c7-d0445f6ca522",
   "metadata": {},
   "source": [
    "# Bigram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68086f6d-9c51-4b37-afb2-d033a8c4971e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.8786, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "        \"\"\"Bigram model, see Karpathy's previous series of videos.\"\"\"\n",
    "        super().__init__()\n",
    "        # Tokens read off the logits for the next token from a lookup table\n",
    "        # Token embedding table has size (vocab_size, vocab_size)\n",
    "        # The way it works is that the input, say 24 (the first one in xb above) will take the 24th row of this\n",
    "        # embedding table. \n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        \"\"\"Forward pass. Takes `idx` and `targets` which are both `(B, T)` tensors of integers.\n",
    "        Here `B` is the batch_size and `T` should be the block/context length.\"\"\"\n",
    "        # PyTorch will grab the row corresponding to the indices provided and return logits in\n",
    "        # the shape (batch, time, channel). Here batch=4, time=8, channel=65 (vocab size)\n",
    "        # The logits here are like the scores for the next token in the sequence\n",
    "        logits = self.token_embedding_table(idx)  # (B, T, C)\n",
    "\n",
    "        # Negative log-likelihood loss (cross-entropy). Importantly, when working with multi-dimensional inputs, \n",
    "        # PyTorch's documentation https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss\n",
    "        # mentions that it requires dimensions (B, C, T) using our notation. A simpler alternative is to simply shape it to (B*T, C)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        \"\"\"Here `idx` is the current context of tokens in some batch, so it is `(B, T)`. This function will continue the generation\n",
    "        one by one, for both the B and T dimensions. It keeps doing this until max_new_tokens.\"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, loss = self(idx)   # Get the predictions (calls forward(idx, targets=None))\n",
    "            logits = logits[:, -1, :]  # (B, T, C) --> (B, C) we focus only on the last \"time step\" (last token in the context)\n",
    "            probs = F.softmax(logits, dim=-1)  # Use Softmax to get probabilities. (B, C)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample using the probabilities (B, 1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # append the sampled index to the running sequence (B, T+1)\n",
    "        return idx\n",
    "\n",
    "\n",
    "bigram = BigramLanguageModel(vocab_size)\n",
    "logits, loss = bigram(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65c959b3-8ea0-4f68-80a3-d0f51d92931f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
      "wnYWmnxKWWev-tDqXErVKLgJ\n"
     ]
    }
   ],
   "source": [
    "# Sample from the model\n",
    "print(\n",
    "    decode(\n",
    "        bigram.generate(\n",
    "            idx=torch.zeros((1, 1), dtype=torch.long),\n",
    "            max_new_tokens=100\n",
    "        )[0].tolist()   # use [0] to pluck out the single batch dimension (we are sending in [[0]] with B=T=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cf96c-48ea-41a7-be26-61d3d162b644",
   "metadata": {},
   "source": [
    "Of course, this is silly. We are feeding in an entire context but really just using the last token to predict the next (see `logits[:, -1, :]`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26fd750-c613-442d-9093-a72802961ab6",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3504eda1-e67e-46a4-bb76-514c35b723f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(bigram.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8f60afc-035e-45ca-bd5e-45f024f3dff7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4955925941467285\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for steps in range(10000):\n",
    "\n",
    "    # Sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = bigram(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf1116e-7ba5-4375-9582-948a8cca0c79",
   "metadata": {},
   "source": [
    "# Generate After Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa886f95-0892-4738-8d45-99f05c7e2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "BOMy is, mumot me bthenindsoferlle cardethe le h. sps t theleriny hacl fougarke,\n",
      "\n",
      "Angllll a hieald lo d,\n",
      "Thade bof jak rend\n",
      "Thid weme w, aithefithe h tes s tomeeseanmpotl'de y l, t,\n",
      "Goutandos t tof al o ad, prsthoneirermenicisull gsewd o re, myofary pef s'HNORicepim th, thit y h,\n",
      "Cothor nsethat bre, lly, farotodis f vel cld, minounged tithit:\n",
      "O:\n",
      "\n",
      "And;\n",
      "DAppr.\n",
      "\n",
      "VI cerer e: t\n",
      "Wipe ght cou FLOLOF ve\n"
     ]
    }
   ],
   "source": [
    "# Sample from the model\n",
    "print(\n",
    "    decode(\n",
    "        bigram.generate(\n",
    "            idx=torch.zeros((1, 1), dtype=torch.long),\n",
    "            max_new_tokens=400\n",
    "        )[0].tolist()   # use [0] to pluck out the single batch dimension (we are sending in [[0]] with B=T=1\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea0b76-d4d5-44a6-b393-7b21fda26fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
